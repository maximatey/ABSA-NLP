{"cells":[{"cell_type":"code","metadata":{"source_hash":"18292628","execution_start":1733950819012,"execution_millis":276097,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"d1896050309a4626be45a7fd2a785016","deepnote_cell_type":"code"},"source":"!pip install gdown\n!pip install datasets\n!pip install transformers\n\n\n!gdown --folder \"https://drive.google.com/drive/folders/17S2rfDOzBDvzBbZCrPMXgHashy6ob3Lz?usp=sharing\"\n!gdown --folder \"https://drive.google.com/drive/folders/1-BRcY-RUeZe0gvUVAdekkzJlxF4Sdz43?usp=sharing\"\n!gdown --folder \"https://drive.google.com/drive/folders/1GB_3zmE8j-iD7Fdg2wKtgv32IeiDxdMG?usp=sharing\"\n!gdown --folder \"https://drive.google.com/drive/folders/1hMV9W-Gybh8t0e131G7b-BXzQL4kJrdY?usp=sharing\"\n!gdown --folder \"https://drive.google.com/drive/folders/1RL5gUz_Sc_-EfNPmqKala47j3Mt_ODpn?usp=sharing\"","block_group":"d1896050309a4626be45a7fd2a785016","execution_count":6,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: beautifulsoup4 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: requests[socks] in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: filelock in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gdown) (3.8.0)\nRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: soupsieve>1.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests[socks]->gdown) (2022.9.24)\nCollecting PySocks!=1.5.7,>=1.5.6\n  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\nInstalling collected packages: PySocks, gdown\nSuccessfully installed PySocks-1.7.1 gdown-5.2.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting datasets\n  Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (1.23.4)\nCollecting requests>=2.32.2\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash\n  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: packaging in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from datasets) (21.3)\nCollecting pyarrow>=15.0.0\n  Downloading pyarrow-18.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.23.0 in /root/venv/lib/python3.9/site-packages (from datasets) (0.26.5)\nRequirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (2024.2.0)\nCollecting multiprocess<0.70.17\n  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tqdm>=4.66.3\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /root/venv/lib/python3.9/site-packages (from datasets) (6.0.2)\nCollecting aiohttp\n  Downloading aiohttp-3.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (3.8.0)\nCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->datasets) (4.4.0)\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2022.9.24)\nRequirement already satisfied: charset-normalizer<4,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.1.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: tzdata>=2022.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas->datasets) (2022.5)\nRequirement already satisfied: pytz>=2020.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas->datasets) (2022.5)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nInstalling collected packages: xxhash, tqdm, requests, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.1\n    Not uninstalling tqdm at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'tqdm'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.28.1\n    Not uninstalling requests at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 9.0.0\n    Not uninstalling pyarrow at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.5.1\n    Not uninstalling dill at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'dill'. No files were found to uninstall.\nSuccessfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 async-timeout-5.0.1 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0 yarl-1.18.3\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: transformers in /root/venv/lib/python3.9/site-packages (4.47.0)\nRequirement already satisfied: pyyaml>=5.1 in /root/venv/lib/python3.9/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: tqdm>=4.27 in /root/venv/lib/python3.9/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: filelock in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (3.8.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /root/venv/lib/python3.9/site-packages (from transformers) (0.26.5)\nRequirement already satisfied: safetensors>=0.4.1 in /root/venv/lib/python3.9/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: numpy>=1.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (1.23.4)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from transformers) (21.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /root/venv/lib/python3.9/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: regex!=2019.12.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (2022.9.13)\nRequirement already satisfied: requests in /root/venv/lib/python3.9/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.4.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRetrieving folder contents\nProcessing file 1-EmHRyAC-FV5ksKpc9HghoxM6CHlELXa config.json\nProcessing file 1-E5jW35scprbyiaFAIB_82_WqJIwhO0O model.safetensors\nProcessing file 1-Lt9gp4-NdFT7DsI0oWM-ycEu0P6OhTZ special_tokens_map.json\nProcessing file 1-IQ03A77GhJh-WMGy_wNEjAuXQCjP--U tokenizer_config.json\nProcessing file 1-O4yHU9Xq2QoD9KwKWEBToFM9MjyZu_b tokenizer.json\nProcessing file 1-01K0D6jNR8-6F1LtnRLfUXwXmXNL3Wb vocab.txt\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1-EmHRyAC-FV5ksKpc9HghoxM6CHlELXa\nTo: /datasets/_deepnote_work/results_bert_20241211_134025/config.json\n100%|██████████████████████████████████████████| 615/615 [00:00<00:00, 1.92MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1-E5jW35scprbyiaFAIB_82_WqJIwhO0O\nFrom (redirected): https://drive.google.com/uc?id=1-E5jW35scprbyiaFAIB_82_WqJIwhO0O&confirm=t&uuid=8f3aca3f-6e06-4e63-b91f-ef7fe3af6646\nTo: /datasets/_deepnote_work/results_bert_20241211_134025/model.safetensors\n100%|█████████████████████████████████████████| 268M/268M [00:01<00:00, 212MB/s]\nError:\n\n\t[Errno 5] Input/output error\n\nTo report issues, please visit https://github.com/wkentaro/gdown/issues.\nRetrieving folder contents\nProcessing file 1-PJe8xn9dJd30iOFaMUdxUnvzv1DTuEp special_tokens_map.json\nProcessing file 1-HrAI6c5Y4yOLcPBEVhBsA5Nn-b3mzSz tokenizer_config.json\nProcessing file 1-VdL5OOXn2miolKrmVhjXdfW4Xa3i9oW tokenizer.json\nProcessing file 1-DFoHRMJM7uI5KlI3lhSt_B1Bgehby4y vocab.txt\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1-PJe8xn9dJd30iOFaMUdxUnvzv1DTuEp\nTo: /datasets/_deepnote_work/tokenizer_bert_base/special_tokens_map.json\n100%|███████████████████████████████████████████| 125/125 [00:00<00:00, 399kB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-HrAI6c5Y4yOLcPBEVhBsA5Nn-b3mzSz\nTo: /datasets/_deepnote_work/tokenizer_bert_base/tokenizer_config.json\n100%|██████████████████████████████████████| 1.19k/1.19k [00:00<00:00, 4.44MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-VdL5OOXn2miolKrmVhjXdfW4Xa3i9oW\nTo: /datasets/_deepnote_work/tokenizer_bert_base/tokenizer.json\n100%|████████████████████████████████████████| 712k/712k [00:00<00:00, 64.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-DFoHRMJM7uI5KlI3lhSt_B1Bgehby4y\nTo: /datasets/_deepnote_work/tokenizer_bert_base/vocab.txt\n100%|████████████████████████████████████████| 232k/232k [00:00<00:00, 39.4MB/s]\nDownload completed\nRetrieving folder contents\nProcessing file 1--2mcTPl9bLuMMTyqfeGFJWPd9KsZsOC config.json\nProcessing file 1-3az06eKkXPffpP-DN-N_ub5c8MLEnp4 model.safetensors\nProcessing file 1-AHQ25UjTukUkd_k8OiVlsQ1VjOOMf_3 training_args.bin\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1--2mcTPl9bLuMMTyqfeGFJWPd9KsZsOC\nTo: /datasets/_deepnote_work/model_task3_bert/config.json\n100%|██████████████████████████████████████████| 881/881 [00:00<00:00, 2.00MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1-3az06eKkXPffpP-DN-N_ub5c8MLEnp4\nFrom (redirected): https://drive.google.com/uc?id=1-3az06eKkXPffpP-DN-N_ub5c8MLEnp4&confirm=t&uuid=1e02b6bf-e761-4f24-bc69-257c5283c464\nTo: /datasets/_deepnote_work/model_task3_bert/model.safetensors\n100%|█████████████████████████████████████████| 438M/438M [00:04<00:00, 103MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-AHQ25UjTukUkd_k8OiVlsQ1VjOOMf_3\nTo: /datasets/_deepnote_work/model_task3_bert/training_args.bin\n100%|██████████████████████████████████████| 5.24k/5.24k [00:00<00:00, 14.8MB/s]\nDownload completed\nRetrieving folder contents\nProcessing file 1-9UPw7uFQpVUhPOfAo_KPdAqMlFUL2KK config.json\nProcessing file 1-690M0NlarhAWuMVVcOaGRgZXa-5U00R model.safetensors\nProcessing file 1-DRp6Skkzd4WTNFYw9jxHe9WocARGPfP special_tokens_map.json\nProcessing file 1-CDwrLQ_zyZdEhuFn5r9EQb9Vae58I9Z tokenizer_config.json\nProcessing file 1-FkNYs4xvZB5XO1pQ1UQhVAj7fzOeZ_V tokenizer.json\nProcessing file 1-1bf-L11yGgAvBdg3g_BvRJAW3Vv9mjO vocab.txt\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1-9UPw7uFQpVUhPOfAo_KPdAqMlFUL2KK\nTo: /datasets/_deepnote_work/results_bert_20241211_151018/config.json\n100%|██████████████████████████████████████████| 827/827 [00:00<00:00, 3.04MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1-690M0NlarhAWuMVVcOaGRgZXa-5U00R\nFrom (redirected): https://drive.google.com/uc?id=1-690M0NlarhAWuMVVcOaGRgZXa-5U00R&confirm=t&uuid=8bcc03be-a5f4-4c4a-9b1d-3346fbffc990\nTo: /datasets/_deepnote_work/results_bert_20241211_151018/model.safetensors\n100%|█████████████████████████████████████████| 431M/431M [00:02<00:00, 186MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-DRp6Skkzd4WTNFYw9jxHe9WocARGPfP\nTo: /datasets/_deepnote_work/results_bert_20241211_151018/special_tokens_map.json\n100%|███████████████████████████████████████████| 125/125 [00:00<00:00, 457kB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-CDwrLQ_zyZdEhuFn5r9EQb9Vae58I9Z\nTo: /datasets/_deepnote_work/results_bert_20241211_151018/tokenizer_config.json\n100%|██████████████████████████████████████| 1.19k/1.19k [00:00<00:00, 4.91MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-FkNYs4xvZB5XO1pQ1UQhVAj7fzOeZ_V\nTo: /datasets/_deepnote_work/results_bert_20241211_151018/tokenizer.json\n100%|████████████████████████████████████████| 669k/669k [00:00<00:00, 54.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-1bf-L11yGgAvBdg3g_BvRJAW3Vv9mjO\nTo: /datasets/_deepnote_work/results_bert_20241211_151018/vocab.txt\n100%|████████████████████████████████████████| 213k/213k [00:00<00:00, 37.3MB/s]\nDownload completed\nRetrieving folder contents\nProcessing file 1-4XO0U9OzKYYBMndKJEZ3-zpo7itXoni config.json\nProcessing file 1-A-SuKIsP6MtlqrR_b3OCk6BtGt6ie2Q model.safetensors\nProcessing file 1-8erxg8wYZbRkPNClME-c63it-_OUaf7 special_tokens_map.json\nProcessing file 1-2XN7CuyTWpMgSaohXGJCFa1142_AoYb tokenizer_config.json\nProcessing file 1-1psXecI989Oq6VOvgw9OMzDkPri5w0z tokenizer.json\nProcessing file 1-9TT-GmNTRSmjwpsjYIOJA65tXL9NJWu vocab.txt\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1-4XO0U9OzKYYBMndKJEZ3-zpo7itXoni\nTo: /datasets/_deepnote_work/results_bert_20241211_205040/config.json\n100%|██████████████████████████████████████████| 827/827 [00:00<00:00, 2.88MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1-A-SuKIsP6MtlqrR_b3OCk6BtGt6ie2Q\nFrom (redirected): https://drive.google.com/uc?id=1-A-SuKIsP6MtlqrR_b3OCk6BtGt6ie2Q&confirm=t&uuid=20da72c3-4288-4d84-962d-9f189d36d032\nTo: /datasets/_deepnote_work/results_bert_20241211_205040/model.safetensors\n100%|████████████████████████████████████████| 431M/431M [00:09<00:00, 45.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-8erxg8wYZbRkPNClME-c63it-_OUaf7\nTo: /datasets/_deepnote_work/results_bert_20241211_205040/special_tokens_map.json\n100%|███████████████████████████████████████████| 125/125 [00:00<00:00, 471kB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-2XN7CuyTWpMgSaohXGJCFa1142_AoYb\nTo: /datasets/_deepnote_work/results_bert_20241211_205040/tokenizer_config.json\n100%|██████████████████████████████████████| 1.19k/1.19k [00:00<00:00, 5.18MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-1psXecI989Oq6VOvgw9OMzDkPri5w0z\nTo: /datasets/_deepnote_work/results_bert_20241211_205040/tokenizer.json\n100%|████████████████████████████████████████| 669k/669k [00:00<00:00, 59.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-9TT-GmNTRSmjwpsjYIOJA65tXL9NJWu\nTo: /datasets/_deepnote_work/results_bert_20241211_205040/vocab.txt\n100%|████████████████████████████████████████| 213k/213k [00:00<00:00, 32.9MB/s]\nDownload completed\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/b580da88-578f-4753-ba1b-c1f0f773ccba","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e20942e0","execution_start":1733954055800,"execution_millis":0,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"12fa400c932d4946854d62a4ec0b9302","deepnote_cell_type":"code"},"source":"ENTITY_ATTRIBUTE_PAIRS = ['FOOD,QUALITY', 'RESTAURANT,GENERAL', 'FOOD,STYLE OPTIONS', 'FOOD,PRICES', 'DRINKS,STYLE OPTIONS', 'SERVICE,GENERAL', 'RESTAURANT,PRICES', 'DRINKS,QUALITY', 'DRINKS,PRICES', 'LOCATION,GENERAL', 'AMBIENCE,GENERAL', 'RESTAURANT,MISCELLANEOUS']","block_group":"376ccec06a3142518963fe2faae9f769","execution_count":65,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cbb57a36747e4013b12df706681675d0","deepnote_cell_type":"text-cell-h1"},"source":"# Pipeline 1: Entity - Attribute Extraction","block_group":"d824bc70449042d89ec43ad127deb8c6"},{"cell_type":"code","metadata":{"source_hash":"8de9ca91","execution_start":1733954058249,"execution_millis":1782,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"d86e1dcedcdd48138f05f0800a9cbc6f","deepnote_cell_type":"code"},"source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_path = \"./results_bert_20241211_134025\"\n\n# Load the model and tokenizer\nmodel_load = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer_load = AutoTokenizer.from_pretrained(model_path)\n\nprint(\"Model and tokenizer loaded successfully!\")","block_group":"648de01a502547b78491ca95a1615668","execution_count":66,"outputs":[{"name":"stdout","text":"Model and tokenizer loaded successfully!\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/c8379d15-1524-4ab0-83c3-1c303fccb79c","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e3afafea","execution_start":1733954063575,"execution_millis":1307,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"93298a9f4e2f4b7f9d037bc48bfc7da5","deepnote_cell_type":"code"},"source":"import torch\nimport numpy as np\n\ndef predict_categories(text, model, tokenizer, entity_attribute_combinations):\n    \"\"\"\n    Given a text string, use the model to predict which entity-attribute categories\n    are present. Returns a list of predicted categories.\n    \"\"\"\n    # We'll store predictions here\n    predicted_pairs = []\n\n    # Iterate over all possible entity-attribute combinations\n    for pair in entity_attribute_combinations:\n        # Tokenize the text with the given pair as the \"label\"\n        tokens = tokenizer(text, pair, return_tensors=\"pt\", truncation=True, padding=True)\n        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n\n        # Run inference\n        with torch.no_grad():\n            outputs = model(**tokens)\n            logits = outputs.logits\n            predicted_label = np.argmax(logits.cpu().numpy(), axis=-1)[0]\n\n        # If the model predicts 1, include this pair in the results\n        if predicted_label == 1:\n            # Format as in your code: upper, replacing space with underscores and commas with '#'\n            formatted_pair = pair.upper().replace(\",\", \"#\").replace(\" \", \"_\")\n            predicted_pairs.append(formatted_pair)\n\n    return predicted_pairs\n\n\ntext_input = \"The pizza crust was the best and the service was a bitbad.\"\npredicted = predict_categories(text_input, model_load, tokenizer_load, ENTITY_ATTRIBUTE_PAIRS)\nprint(\"Predicted categories:\", predicted)","block_group":"4c40ee6ee7f44acb847e8f26f56e1b1b","execution_count":67,"outputs":[{"name":"stdout","text":"Predicted categories: ['FOOD#QUALITY', 'SERVICE#GENERAL']\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e332bacd-ce71-4779-a374-44e00e3a93e5","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"55d9c78e96074022b2f464fd6f56248c","deepnote_cell_type":"text-cell-h1"},"source":"# Pipeline 2: Opinion Target Extraction","block_group":"124cd7a0936b48d2afd8411522a1cc24"},{"cell_type":"code","metadata":{"source_hash":"1374a44d","execution_start":1733954068571,"execution_millis":2,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"3fa05d6a6d4849ea8cf71043be683c88","deepnote_cell_type":"code"},"source":"label_map = {\"O\": 0, \"B-OTE\": 1, \"I-OTE\": 2}","block_group":"9a8f03bee3e440249ade236df36541f7","execution_count":68,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"df203e39","execution_start":1733954070708,"execution_millis":1968,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"3600612171ed42a7be7352cd536b6f25","deepnote_cell_type":"code"},"source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_path_2 = \"./results_bert_20241211_205040\"\n\n# Load the model and tokenizer\nmodel_load_2 = AutoModelForTokenClassification.from_pretrained(model_path_2)\ntokenizer_load_2 = AutoTokenizer.from_pretrained(model_path_2)\n\nprint(\"Model and tokenizer loaded successfully!\")","block_group":"446cfc37b9334114b468585f6d28b3fb","execution_count":69,"outputs":[{"name":"stdout","text":"Model and tokenizer loaded successfully!\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/850f4533-1db5-4e24-b044-9e86661ceef2","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"fda769ec","execution_start":1733954130692,"execution_millis":0,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"a93cfee7e4334b94a5bc3a81aab1b144","deepnote_cell_type":"code"},"source":"def predict_ote(model, tokenizer, text, aspect, label_map):\n    model.eval()  # Set model to evaluation mode\n\n    # Tokenize the input text\n    tokens = tokenizer(\n        aspect,\n        text,\n        truncation=True,\n        padding=True,\n        return_tensors=\"pt\",\n        return_offsets_mapping=True  # Include offsets\n    )\n    input_ids = tokens[\"input_ids\"]\n    attention_mask = tokens[\"attention_mask\"]\n    offset_mapping = tokens[\"offset_mapping\"][0].tolist()  # Offset mapping for tokens\n\n    # Perform inference\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits  # Get logits for each token\n        predictions = torch.argmax(logits, dim=2)  # Get the predicted class for each token\n\n    # Convert predictions to labels\n    label_map_reverse = {v: k for k, v in label_map.items()}  # Reverse the label map\n    predicted_labels = [label_map_reverse[label.item()] for label in predictions[0]]\n\n    # Convert tokens to readable format and filter OTEs\n    tokens_list = tokenizer.convert_ids_to_tokens(input_ids[0])\n\n    # Determine the index of the first text token (ignoring aspect tokens)\n    text_start_index = len(tokenizer(aspect, add_special_tokens=False)[\"input_ids\"])\n\n    ote_tokens = []\n    ongoing_ote = False  # Track if we're inside a valid OTE span\n\n    for idx, (token, label, (start, end)) in enumerate(zip(tokens_list, predicted_labels, offset_mapping)):\n        if start == 0 and end == 0:  # Skip special tokens like [CLS] or [SEP]\n            continue\n\n        if idx < text_start_index:  # Ignore tokens before the main text\n            continue\n\n        if label == \"B-OTE\":  # Start a new OTE span\n            ongoing_ote = True\n            ote_tokens.append({\n                \"token\": token,\n                \"start\": start,\n                \"end\": end\n            })\n        elif label == \"I-OTE\" and ongoing_ote:  # Continue a valid OTE span\n            ote_tokens[-1][\"token\"] += token.replace(\"##\", \"\")  # Merge sub-token (if any)\n            ote_tokens[-1][\"end\"] = end  # Update the end offset\n        else:\n            ongoing_ote = False  # Reset if not part of an OTE span\n\n    return ote_tokens\n","block_group":"c4a60ed4e2bb49a690e4cd2b81352eb6","execution_count":71,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"4435e26a","execution_start":1733954135973,"execution_millis":212,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"aeac54eb5ecd46ac837901aab70582fe","deepnote_cell_type":"code"},"source":"predict_ote(model_load_2, tokenizer_load_2, \"The pizza crust was the best and the service was a bit bad\", \"SERVICE#GENERAL\", label_map)","block_group":"4d87997b44664b94898539ec6ca0653b","execution_count":72,"outputs":[{"output_type":"execute_result","execution_count":72,"data":{"text/plain":"[{'token': 'service', 'start': 37, 'end': 44}]"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/5ca9b1ab-5217-4156-9af3-7a53ce3aeb83","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"251a79795f284e778039bf211eb5078a","deepnote_cell_type":"text-cell-h1"},"source":"# Pipeline 3: Sentiment Polarity","block_group":"173d597eeb0f45e48c7fd54970e6b8f9"},{"cell_type":"code","metadata":{"source_hash":"e27d8a3d","execution_start":1733954138184,"execution_millis":2557,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"6ebc9370fffe4d2ea669e940ed5f78bb","deepnote_cell_type":"code"},"source":"from transformers import AutoModelForSequenceClassification,AutoTokenizer\nfrom transformers import TrainingArguments, Trainer\n\nmodel_task3 = AutoModelForSequenceClassification.from_pretrained(\"./model_bert_task3\")\ntokenizer_task3 = AutoTokenizer.from_pretrained(\"./tokenizer_bert_base\")\n\ndef preprocess_function(examples):\n    return tokenizer_task3(examples[\"text\"], padding=\"max_length\", truncation=True,  return_tensors=\"pt\")","block_group":"9ba426a7687e4fb79adb07123e667473","execution_count":73,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"6a523448","execution_start":1733954142347,"execution_millis":2,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"62f52c5f139540899ca42dd98f9c998a","deepnote_cell_type":"code"},"source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport torch\n\n# Assuming model and tokenizer are already loaded\ndef predict_polarity(model, sentence, entity, attribute, target, tokenizer):\n    model.eval()\n\n    # Prepend the entity, attribute, and target to the sentence for aspect-driven sentiment analysis\n    input_text = f\"{target} {attribute} {entity} [SEP] {sentence}\"\n\n    # Tokenize the input text\n    encoded_inputs = tokenizer(\n        input_text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_tensors=\"pt\"\n    )\n\n    input_ids = encoded_inputs[\"input_ids\"]\n    attention_mask = encoded_inputs[\"attention_mask\"]\n\n    # Perform inference\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    prediction = torch.argmax(logits, dim=1).item()\n\n    return prediction\n","block_group":"2abf608b794f4bcf9d90bedb5a212408","execution_count":74,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1ce3d907","execution_start":1733954145619,"execution_millis":1,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"056106577d234a67b6fafc409dde286d","deepnote_cell_type":"code"},"source":"def ABSA(sentence_input):\n    predicted_categories = predict_categories(sentence_input, model_load, tokenizer_load, ENTITY_ATTRIBUTE_PAIRS)\n    predicted_targets = []\n    \n    # Collect each aspect's tokens for the predicted categories\n    for category in predicted_categories:\n        tokens = predict_ote(model_load_2, tokenizer_load_2, sentence_input, category, label_map)\n        for token in tokens:\n            predicted_targets.append({\"aspect\": category, \"token\": token[\"token\"]})\n\n    # Process each aspect and target for sentiment analysis\n    for item in predicted_targets:\n        entity = item[\"aspect\"].split('#')[0]\n        attribute = item[\"aspect\"].split('#')[1]\n        target = item['token']\n\n        prediction = predict_polarity(model_task3, sentence_input, entity, attribute, target, tokenizer_task3)\n\n        print(f\"Input: {sentence_input}\")\n        print(f\"Category: {entity}#{attribute}\")\n        print(f\"Target: {target}\")\n        if prediction == 0:\n            print(\"Polarity: negative\")\n        elif prediction == 1:\n            print(\"Polarity: neutral\")\n        else:\n            print(\"Polarity: positive\")\n\n        print()","block_group":"5e1fc01a72ec453b99fe5b4a7a532d27","execution_count":75,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"66d97f60","execution_start":1733954376804,"execution_millis":10589,"execution_context_id":"c96f23fe-c365-4df7-baf7-e1812b6e6d51","cell_id":"5aeb2fcae3294c849b15116b32c49211","deepnote_cell_type":"code"},"source":"sentence_input = input(\"Enter a sentence: \")\nABSA(sentence_input)","block_group":"fbf349cee67c4ab8ba5dab284c0901f8","execution_count":84,"outputs":[{"name":"stdout","text":"Input: The steak was expensive\nCategory: FOOD#PRICES\nTarget: steak\nPolarity: positive\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/7eadd94a-f01d-4bae-be5b-59affcb30b44","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1f59986d-c4b4-4654-929d-a1205f783ae5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"6fd9878b7af648bb855e43217338702a"}}